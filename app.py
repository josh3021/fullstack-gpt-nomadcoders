import os

import streamlit as st
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate
from langchain.schema import AIMessage, HumanMessage
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.runnable.base import RunnableLambda
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS

st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
)


class Memory:
    def __init__(self):
        self.memory = ConversationBufferMemory(return_messages=True)

    def save_memory(self, input_text, output_text):
        self.memory.save_context(
            {"input": input_text},
            {"output": output_text}
        )

    def load_memory_variables(self):
        return self.memory.load_memory_variables({})

    def get_chat_history(self):
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
        memorized_messages = self.memory.chat_memory.messages
        history = []
        for memorized_message in memorized_messages:
            if isinstance(memorized_message, HumanMessage):
                history.append(f"Human: {memorized_message.content}")
            elif isinstance(memorized_message, AIMessage):
                history.append(f"AI: {memorized_message.content}")
        return "\n".join(history)


class ChatCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        self.message = ''
        self.message_box = None

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()
        self.message = ""

    def on_llm_new_token(self, token: str, *args, **kwargs):
        self.message += token
        if self.message_box is not None:
            self.message_box.markdown(self.message)

    def on_llm_end(self, *args, **kwargs):
        # ìŠ¤íŠ¸ë¦¬ë°ì´ ëë‚˜ë©´ ë©”ì‹œì§€ ì €ì¥
        save_message(self.message, 'ai')


def send_message(msg_content, role, save=True):
    with st.chat_message(role):
        st.markdown(msg_content)
    if save:
        save_message(msg_content, role)


def save_message(msg_content, role):
    if 'messages' not in st.session_state:
        st.session_state['messages'] = []
    st.session_state['messages'].append({"message": msg_content, "role": role})


def paint_history():
    if 'messages' in st.session_state:
        for message in st.session_state['messages']:
            send_message(message['message'], message['role'], False)


@st.cache_data(show_spinner="Embedding file...")
def embed_file(original_file):
    os.makedirs("./.cache/files", exist_ok=True)
    file_content = original_file.read()
    file_path = f"./.cache/files/{original_file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    cache_dir = LocalFileStore(f"./.cache/embeddings/{original_file.name}")

    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )

    # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)

    # ì„ë² ë”© ìƒì„±
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
        embeddings, cache_dir)

    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()

    return retriever


def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)


# ë©”ì¸ UI
st.title("DocumentGPT")

st.markdown(
    """
    Welcome!
                
    Use this chatbot to ask questions to an AI about your files!
    
    Upload your files on the sidebar.
    """
)

with st.sidebar:
    api_key_input = st.text_input(
        'OpenAI APIí‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.',
        type="password",
        help="sk-ë¡œ ì‹œì‘í•˜ëŠ” OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    )

    # íŒŒì¼ ì—…ë¡œë“œ
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
    )

    st.link_button('Go to Github Repository',
                   "https://github.com/josh3021/fullstack-gpt-nomadcoders")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state['messages'] = []


if api_key_input and file:
    try:
        # íŒŒì¼ì´ë‚˜ API í‚¤ê°€ ë³€ê²½ë˜ë©´ ë©”ì‹œì§€ ì´ˆê¸°í™”
        if 'current_file' not in st.session_state or st.session_state.get('current_file') != file:
            st.session_state['messages'] = []
            st.session_state['current_file'] = file

        # íŒŒì¼ ì„ë² ë”©
        retriever = embed_file(file)

        # ì¤€ë¹„ ì™„ë£Œ ë©”ì‹œì§€ (í•œ ë²ˆë§Œ í‘œì‹œ)
        if len(st.session_state['messages']) == 0:
            send_message("íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! íŒŒì¼ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”...", "ai", False)

        # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
        paint_history()

        # ì‚¬ìš©ì ì…ë ¥
        user_input = st.chat_input("íŒŒì¼ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”...")

        if user_input:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í‘œì‹œ
            send_message(user_input, 'human')

            # ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„± (ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±)
            callback_handler = ChatCallbackHandler()

            # LLM ì´ˆê¸°í™”
            llm = ChatOpenAI(
                api_key=api_key_input,
                model="gpt-4o-mini",
                temperature=0.1,
                streaming=True,
                callbacks=[callback_handler]
            )

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            prompt = ChatPromptTemplate.from_messages([
                ("system",
                 """
                    Answer the question using ONLY the following context.
                    If you don't know the answer just say you don't know.
                    Don't make anything up.
                    And most important thing is ANSWER ONLY IN KOREAN!

                    Context: {context}
                """),
                ("human", "{question}")
            ])

            # ì²´ì¸ ìƒì„±
            chain = (
                {
                    "context": retriever | RunnableLambda(format_docs),
                    "question": RunnablePassthrough()
                }
                | prompt
                | llm
            )

            # AI ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
            with st.chat_message('ai'):
                response = chain.invoke(user_input)

    except Exception as e:
        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        st.info("API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

elif not api_key_input:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

elif not file:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
