{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdade7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Question 1: Is Aaronson guilty?\n",
      "==================================================\n",
      "Answer:\n",
      "He was guilty of the crimes he was charged with.\n",
      "\n",
      "==================================================\n",
      "Question 2: What message did he write in the table?\n",
      "==================================================\n",
      "Answer:\n",
      "He wrote \"2+2=5\" in the dust on the table.\n",
      "\n",
      "==================================================\n",
      "Question 3: Who is Julia?\n",
      "==================================================\n",
      "Answer:\n",
      "Julia is a character mentioned in the context, who Winston loves and feels a deep connection to, especially during moments of emotional turmoil.\n",
      "\n",
      "==================================================\n",
      "Final Chat History:\n",
      "==================================================\n",
      "Human: Is Aaronson guilty?\n",
      "AI: He was guilty of the crimes he was charged with.\n",
      "Human: What message did he write in the table?\n",
      "AI: He wrote \"2+2=5\" in the dust on the table.\n",
      "Human: Who is Julia?\n",
      "AI: Julia is a character mentioned in the context, who Winston loves and feels a deep connection to, especially during moments of emotional turmoil.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "    def save_memory(self, input_text, output_text):\n",
    "        self.memory.save_context(\n",
    "            {\"input\": input_text},\n",
    "            {\"output\": output_text}\n",
    "        )\n",
    "\n",
    "    def load_memory_variables(self):\n",
    "        return self.memory.load_memory_variables({})\n",
    "\n",
    "    def get_chat_history(self):\n",
    "        \"\"\"채팅 히스토리를 문자열로 반환\"\"\"\n",
    "        messages = self.memory.chat_memory.messages\n",
    "        history = []\n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                history.append(f\"Human: {message.content}\")\n",
    "            elif isinstance(message, AIMessage):\n",
    "                history.append(f\"AI: {message.content}\")\n",
    "        return \"\\n\".join(history)\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, file_path):\n",
    "        self.loader = UnstructuredFileLoader(file_path)\n",
    "        self.splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            separator=\"\\n\",\n",
    "            chunk_size=600,\n",
    "            chunk_overlap=100,\n",
    "        )\n",
    "        self.docs = self.loader.load_and_split(text_splitter=self.splitter)\n",
    "\n",
    "    def get_documents(self):\n",
    "        return self.docs\n",
    "\n",
    "\n",
    "class StuffDocumentsChain:\n",
    "    def __init__(self, docs, memory):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            streaming=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.cache_dir = LocalFileStore('./.cache/')\n",
    "        self.cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "            self.embeddings, self.cache_dir\n",
    "        )\n",
    "        self.vectorstore = FAISS.from_documents(docs, self.cached_embeddings)\n",
    "        self.retriever = self.vectorstore.as_retriever()\n",
    "        self.memory = memory\n",
    "\n",
    "    def create_prompt_with_memory(self, question, context, chat_history=\"\"):\n",
    "        prompt_template = \"\"\"You are a helpful assistant. Answer questions using only the following context and conversation history.\n",
    "          If you don't know the answer just say you don't know, don't make it up.\n",
    "\n",
    "          Previous conversation:\n",
    "          {chat_history}\n",
    "\n",
    "          Context:\n",
    "          {context}\n",
    "\n",
    "          Question: {question}\n",
    "\n",
    "          Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        return prompt_template.format(\n",
    "            chat_history=chat_history,\n",
    "            context=context,\n",
    "            question=question\n",
    "        )\n",
    "\n",
    "    def stuff_documents(self, docs):\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    def invoke(self, question):\n",
    "        relevant_docs = self.retriever.get_relevant_documents(question)\n",
    "        context = self.stuff_documents(relevant_docs)\n",
    "        chat_history = self.memory.get_chat_history()\n",
    "        full_prompt = self.create_prompt_with_memory(\n",
    "            question, context, chat_history)\n",
    "\n",
    "        response = self.llm.invoke([HumanMessage(content=full_prompt)])\n",
    "        self.memory.save_memory(question, response.content)\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    doc = Document('./files/chapter_three.txt')\n",
    "    documents = doc.get_documents()\n",
    "    memory = Memory()\n",
    "    stuff_chain = StuffDocumentsChain(documents, memory)\n",
    "\n",
    "    questions = [\n",
    "        \"Is Aaronson guilty?\",\n",
    "        \"What message did he write in the table?\",\n",
    "        \"Who is Julia?\"\n",
    "    ]\n",
    "\n",
    "    # 각 질문 실행 및 결과 출력\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Question {i}: {question}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        try:\n",
    "            result = stuff_chain.invoke(question)\n",
    "            print(\"Answer:\")\n",
    "            print(result.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error Occured: {e}\")\n",
    "\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(\"Final Chat History:\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(memory.get_chat_history())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
